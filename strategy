【目的 / ゴール】
- 重尾（100–200）を取りこぼさず、未契約店舗の売上を高精度に推定する。
- 回帰器の多様性（木系/線形/GLM）とゲート（分類）を組み合わせ、上側の過小予測を抑制する。

【前提 / データ】
- 目的変数: target_amount_tableau（>0, 下限≈20〜30, 重尾）
- 説明変数: 立地・レビュー・価格・座席数・駅距離・人口など（カテゴリ含む）
- カテゴリ例: CUISINE_CAT_origin（焼肉/寿司/…）
- 推奨検証: GroupKFold（地域/チェーンでリーク抑止）; 本書ではOOFを前提。

============================================================
1. 前段の分類（ゲート）
============================================================
1-1. ラベル化
- 方式A: 3値区間ラベル low/middle/high
  ・low: [20,50), middle: [50,150), high: [150, ∞)
- 方式B: ≈100分位ビン（データ密度に応じた等確率ビニング）

1-2. 学習
- モデル: LightGBM Classifier（multiclass）
- 出力: 各区間の確率ベクトル P(bin=k | x)
- 実装要点:
  ・OOFで P(bin|x) を作成（リーク防止）
  ・クラス不均衡への対策（class_weight、macro F1監視）
  ・未知カテゴリは欠損扱い（pandas category の set_categories）

1-3. 評価（分類）
- 指標: macro F1、クラス別F1/Recall（特に high）、混同行列
- 期待: high の再現↑（後段の上側予測に直結）

============================================================
2. 各“専門家（回帰器）”の学習
============================================================
2-1. ベース回帰器（Level-0）
- LightGBM（GBDT/DART/linear_tree）
- CatBoost（カテゴリ強い）
- 線形系: Ridge / ElasticNet
- GLM: Tweedie（power≈1.2–1.6）または Poisson（logリンク）

2-2. 部分プーリング（任意 / 推奨）
- 大カテゴリ（件数多）: カテゴリ別ローカル回帰器 f_c(x)
- 小カテゴリ（件数少）: 全体グローバル回帰器 g(x)
- 縮約: λ_c = n_c / (n_c + m)（m≈200 目安）
  予測: ŷ_pool = λ_c · f_c(x) + (1−λ_c) · g(x)

2-3. 特徴設計（要点）
- カテゴリ内 z-score/比率化（“そのカテゴリで相対的に高い/低い”）
- CVターゲットエンコード（平滑化 m-推定）
- 距離×人流、評価数×評価点、座席×営業時間などの相互作用

============================================================
3. Mixture（確率重みづけ）による第一次点予測
============================================================
3-1. 方法A（代表値Mixture）
- 各ビンの代表値 r_k（中央値/上位分位）を用意
- 第一次予測: ŷ^(1) = Σ_k P(bin=k|x) · r_k
  → 分布形状（重尾）を素直に反映。単純・堅牢。

3-2. 方法B（専門家Mixture）
- 各ビン/クラスに専用回帰器 f_k(x) を学習
- 第一次予測: ŷ^(1) = Σ_k P(bin=k|x) · f_k(x)
  → 高値域の局所性を表現。データが十分なビンから導入。

============================================================
4. Blending（重み付き平均）
============================================================
4-1. 入力
- ベース回帰器（2章）と Mixture（3章）の OOF 予測行列 Z ∈ R^{N×M}

4-2. 最適化
- 目的関数: Pinball Loss（α≈0.85）  ※上側の過小予測を抑制
- 制約: w_i ≥ 0, Σ_i w_i = 1
- 出力: 最適重み w*（OOFで推定。推論時は同重みで合成）

4-3. 出力
- ŷ_blend = Z · w*

============================================================
5. Stacking（メタ学習）
============================================================
5-1. 入力特徴（リーク防止でOOFのみ）
- ベース/ミクスチャの OOF 予測列
- ゲート確率 P(bin|x)（必要に応じて）
- 少数の強特徴（例: 業態, 立地クラスタ）

5-2. メタ学習器（ロバスト系）
- Quantile回帰（α=0.7–0.9）
- LAD（= L1 / 中央値回帰）
- Huber（外れ値に頑健）

5-3. 出力
- ŷ_stack（メタ学習器の点予測）

============================================================
6. 最終統合 & 微修正
============================================================
6-1. 2重ブレンド
- 入力: ŷ_blend と ŷ_stack
- 目的: Pinball Loss（α≈0.85）; 制約: 非負・総和1
- 出力: ŷ_final = β · ŷ_blend + (1−β) · ŷ_stack

6-2. 高値域残差ブースト（任意）
- 例: r_plus = max(0, y − ŷ_final) を高値域（y≥T）に限定して二段目で学習し加算
- 目的: ピーク/テールの微調整

============================================================
7. 評価 / モニタリング
============================================================
- 分類（ゲート）: macro F1、HighのRecall/F1、混同行列
- 回帰（全体）: Pinball Loss（α≈0.85）、MAE/QuantileMAE
- テール特化: y≥150 のMAE/Recall、上位10% MAE、分布整合（KS距離）
- キャリブレーション: 予測vs実測の分位プロット、PI（予測区間）の被覆率
- データドリフト: 特徴分布/ゲート確率/誤差の時系列監視

============================================================
8. 出力/成果物（データ列・モデル）
============================================================
- ゲート出力: p_low, p_middle, p_high, pred_class, pred_confidence
- 各ベース/ミクスチャ予測: y_hat_*
- ブレンド出力: y_hat_blend
- スタック出力: y_hat_stack
- 最終出力: y_hat_final
- 保存モデル: gate_clf, experts (各回帰器), blender_weights, meta_model
